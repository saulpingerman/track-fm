{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf91ec2b-1cc4-4449-9332-66623845cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user\n",
      "Found 2 parquet files:\n",
      "  data/cleaned_partitioned_ais/year=2025/month=2/day=2/part-0.parquet\n",
      "  data/cleaned_partitioned_ais/year=2025/month=2/day=1/part-0.parquet\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user\n",
    "import glob\n",
    "\n",
    "matches = glob.glob(\"data/cleaned_partitioned_ais/*/*/*/*.parquet\")\n",
    "print(f\"Found {len(matches)} parquet files:\")\n",
    "for p in matches[:10]:\n",
    "    print(\" \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61be75e7-3fe9-4759-9cd4-289f571526c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user\n",
      "🟢 Cell start: loading data\n",
      "🟡 Entering load_cleaned_data()\n",
      "    Looking under /home/ec2-user/data/cleaned_partitioned_ais\n",
      "    Found 2 parquet files\n",
      "    Reading into Polars...\n",
      "🟢 Loaded DataFrame: 19082312 rows, 9 cols\n",
      "🟢 Starting training\n",
      "🟡 Training on device: cuda\n",
      "🟡 Initializing streaming dataset\n",
      "🟢 Dataset init: 3866 vessels in 2.6s\n",
      "🟢 DataLoader ready with batch_size=64\n",
      "🟡 Building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/four_head/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Model built\n",
      "\n",
      "🟡 Epoch 1/100 start\n",
      "    [Epoch 1] Batch 50 NLL -6.6560\n",
      "    [Epoch 1] Batch 100 NLL -8.8237\n",
      "    [Epoch 1] Batch 150 NLL -9.7887\n",
      "    [Epoch 1] Batch 200 NLL -10.2947\n",
      "    [Epoch 1] Batch 250 NLL -10.5902\n",
      "    [Epoch 1] Batch 300 NLL -4.9419\n",
      "    [Epoch 1] Batch 350 NLL -9.1600\n",
      "    [Epoch 1] Batch 400 NLL -10.0367\n",
      "    [Epoch 1] Batch 450 NLL -10.4509\n",
      "    [Epoch 1] Batch 500 NLL -10.6846\n",
      "    [Epoch 1] Batch 550 NLL -10.8268\n",
      "    [Epoch 1] Batch 600 NLL -8.6670\n",
      "    [Epoch 1] Batch 650 NLL -9.9896\n",
      "    [Epoch 1] Batch 700 NLL -10.4759\n",
      "    [Epoch 1] Batch 750 NLL -10.7260\n",
      "    [Epoch 1] Batch 800 NLL -10.8687\n",
      "    [Epoch 1] Batch 850 NLL -7.6541\n",
      "    [Epoch 1] Batch 900 NLL -9.9043\n",
      "    [Epoch 1] Batch 950 NLL -10.4834\n",
      "    [Epoch 1] Batch 1000 NLL -10.7496\n",
      "    [Epoch 1] Batch 1050 NLL -10.8917\n",
      "    [Epoch 1] Batch 1100 NLL -10.9689\n",
      "    [Epoch 1] Batch 1150 NLL -11.0143\n",
      "    [Epoch 1] Batch 1200 NLL -11.0399\n",
      "    [Epoch 1] Batch 1250 NLL -11.0583\n",
      "    [Epoch 1] Batch 1300 NLL -11.0675\n",
      "    [Epoch 1] Batch 1350 NLL -11.0737\n",
      "    [Epoch 1] Batch 1400 NLL -8.8719\n",
      "    [Epoch 1] Batch 1450 NLL -10.3311\n",
      "    [Epoch 1] Batch 1500 NLL -10.7305\n",
      "    [Epoch 1] Batch 1550 NLL -10.9022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 135\u001b[0m\n\u001b[1;32m    132\u001b[0m df \u001b[38;5;241m=\u001b[39m load_cleaned_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cleaned_partitioned_ais\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🟢 Starting training\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 135\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mff_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfourier_m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m    147\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🟢 Training complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(df, seq_len, d_model, nhead, num_layers, ff_hidden, fourier_m, rank, batch_size, lr, epochs, device)\u001b[0m\n\u001b[1;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(pdf \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    119\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 121\u001b[0m total_nll \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    122\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user\n",
    "# ─── 1) Setup imports & module path ──────────────────────────────────────────\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# add your Fourier head module to path\n",
    "fourier_dir = Path.home() / \"repos\" / \"fourier-head\" / \"notebooks\"\n",
    "sys.path.insert(0, str(fourier_dir))\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "from four_head_2D_LR import FourierHead2DLR  # your FourierHead2D_FFT implementation\n",
    "\n",
    "# ─── 2) Data loading helper ─────────────────────────────────────────────────\n",
    "def load_cleaned_data(cleaned_root: str) -> pl.DataFrame:\n",
    "    print(\"🟡 Entering load_cleaned_data()\", flush=True)\n",
    "    p = Path(cleaned_root).resolve()\n",
    "    print(f\"    Looking under {p}\", flush=True)\n",
    "    files = list(p.rglob(\"*.parquet\"))\n",
    "    print(f\"    Found {len(files)} parquet files\", flush=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files under {p}\")\n",
    "    print(\"    Reading into Polars...\", flush=True)\n",
    "    df = pl.read_parquet([str(f) for f in files])\n",
    "    print(f\"🟢 Loaded DataFrame: {df.height} rows, {len(df.columns)} cols\", flush=True)\n",
    "    return df\n",
    "\n",
    "# ─── 3) Streaming dataset ───────────────────────────────────────────────────\n",
    "class AISForecastIterableDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Streams windows of past->next positions, one vessel at a time.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pl.DataFrame, seq_len: int = 10):\n",
    "        print(\"🟡 Initializing streaming dataset\", flush=True)\n",
    "        start = time.time()\n",
    "        # normalize coordinates\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"lat\") / 90.0).alias(\"lat_n\"),\n",
    "            (pl.col(\"lon\") / 180.0).alias(\"lon_n\"),\n",
    "        ])\n",
    "        self.df = df.sort([\"mmsi\", \"timestamp\"])\n",
    "        self.mmsis = self.df[\"mmsi\"].unique().to_list()\n",
    "        self.seq_len = seq_len\n",
    "        print(f\"🟢 Dataset init: {len(self.mmsis)} vessels in {time.time()-start:.1f}s\", flush=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx, m in enumerate(self.mmsis, 1):\n",
    "            grp = self.df.filter(pl.col(\"mmsi\") == m).select([\"lat_n\", \"lon_n\"])\n",
    "            coords = torch.tensor(grp.to_numpy(), dtype=torch.float32)\n",
    "            N = coords.size(0)\n",
    "            if N <= self.seq_len:\n",
    "                continue\n",
    "            for i in range(self.seq_len, N):\n",
    "                past   = coords[i-self.seq_len:i]  # (seq_len,2)\n",
    "                target = coords[i]                 # (2,)\n",
    "                yield past, target\n",
    "            if idx % 500 == 0:\n",
    "                print(f\"    [Dataset] streamed {idx}/{len(self.mmsis)} vessels\", flush=True)\n",
    "\n",
    "# ─── 4) Model definition ────────────────────────────────────────────────────\n",
    "class TransformerForecaster(nn.Module):\n",
    "    def __init__(self, seq_len: int, d_model: int, nhead: int,\n",
    "                 num_layers: int, ff_hidden: int, fourier_m: int, rank: int):\n",
    "        super().__init__()\n",
    "        print(\"🟡 Building model\", flush=True)\n",
    "        self.input_proj = nn.Linear(2, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, ff_hidden)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fh = FourierHead2DLR(dim_input=d_model, num_frequencies=fourier_m,rank=rank)\n",
    "        print(\"🟢 Model built\", flush=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, seq_len, 2), targets: (B, 2)\n",
    "        h = self.input_proj(x) + self.pos_emb.unsqueeze(0)   # (B,S,d_model)\n",
    "        h = self.transformer(h.transpose(0,1))               # (S,B,d_model)\n",
    "        last = h[-1]                                         # (B,d_model)\n",
    "        return self.fh(last, targets)                        # (B,)\n",
    "\n",
    "# ─── 5) Training loop ───────────────────────────────────────────────────────\n",
    "def train(\n",
    "    df: pl.DataFrame,\n",
    "    seq_len: int = 10,\n",
    "    d_model: int = 64,\n",
    "    nhead: int = 4,\n",
    "    num_layers: int = 2,\n",
    "    ff_hidden: int = 128,\n",
    "    fourier_m: int = 8,\n",
    "    rank: int = 4,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 1e-6,\n",
    "    epochs: int = 5,\n",
    "    device: str = None\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"🟡 Training on device: {device}\", flush=True)\n",
    "\n",
    "    ds = AISForecastIterableDataset(df, seq_len=seq_len)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                        drop_last=True, num_workers=0, pin_memory=True)\n",
    "    print(f\"🟢 DataLoader ready with batch_size={batch_size}\", flush=True)\n",
    "\n",
    "    model = TransformerForecaster(seq_len, d_model, nhead, num_layers, ff_hidden, fourier_m, rank)\n",
    "    model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"\\n🟡 Epoch {ep}/{epochs} start\", flush=True)\n",
    "        total_nll = 0.0\n",
    "        count = 0\n",
    "        for batch_i, (xb, yb) in enumerate(loader, 1):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pdf = model(xb, yb)\n",
    "            loss = -(pdf + 1e-12).log().mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            total_nll += loss.item() * xb.size(0)\n",
    "            count += xb.size(0)\n",
    "            if batch_i % 50 == 0:\n",
    "                print(f\"    [Epoch {ep}] Batch {batch_i} NLL {loss.item():.4f}\", flush=True)\n",
    "        avg_nll = total_nll / count\n",
    "        print(f\"🟢 Epoch {ep} done — Avg NLL: {avg_nll:.4f}\", flush=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ─── 6) Run everything ───────────────────────────────────────────────────────\n",
    "print(\"🟢 Cell start: loading data\", flush=True)\n",
    "df = load_cleaned_data(\"data/cleaned_partitioned_ais\")\n",
    "\n",
    "print(\"🟢 Starting training\", flush=True)\n",
    "model = train(\n",
    "    df,\n",
    "    seq_len=20,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=4,\n",
    "    ff_hidden=512,\n",
    "    fourier_m=512,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    epochs=100,\n",
    "    rank = 4\n",
    ")\n",
    "print(\"🟢 Training complete\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d335c0f-7c1f-4bf7-a929-11ead6883e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user\n",
      "Loaded 19,082,312 rows from 2 parquet files\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Config' has no attribute 'set_tbl_opt_activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# ─── 6) Execute end-to-end ──────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    152\u001b[0m df \u001b[38;5;241m=\u001b[39m load_cleaned_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cleaned_partitioned_ais\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mff_hidden\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmse_weight\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# λ\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅  Training finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 116\u001b[0m, in \u001b[0;36mtrain_gmm\u001b[0;34m(df, seq_len, d_model, nhead, num_layers, ff_hidden, num_components, batch_size, lr, epochs, mse_weight, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_gmm\u001b[39m(\n\u001b[1;32m    101\u001b[0m     df: pl\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     device: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    114\u001b[0m ):\n\u001b[1;32m    115\u001b[0m     device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mAISForecastIterableDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    118\u001b[0m         ds, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39m(device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m     model \u001b[38;5;241m=\u001b[39m TransformerForecasterGMM(\n\u001b[1;32m    122\u001b[0m         seq_len, d_model, nhead, num_layers,\n\u001b[1;32m    123\u001b[0m         ff_hidden, num_components, device\n\u001b[1;32m    124\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mAISForecastIterableDataset.__init__\u001b[0;34m(self, df, seq_len)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     38\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tbl_opt_activation\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# let Polars use all cores\u001b[39;00m\n\u001b[1;32m     42\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     43\u001b[0m     df\u001b[38;5;241m.\u001b[39mwith_columns([\n\u001b[1;32m     44\u001b[0m         (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m90\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat_n\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m.\u001b[39msort([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmsi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])        \u001b[38;5;66;03m# heavy\u001b[39;00m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Config' has no attribute 'set_tbl_opt_activation'"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  AIS → Transformer → 2-D GMM forecaster                                 ║\n",
    "# ║  with blended loss = NLL  +  λ·MSE(μ_k , target)                        ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
    "# 0) (optional) run inside /home/ec2-user so data paths stay the same\n",
    "%cd /home/ec2-user\n",
    "\n",
    "# ─── 1) Imports & module path ───────────────────────────────────────────────\n",
    "import sys, time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "# add repo so `GMMHead2D` is importable\n",
    "repo_dir = Path.home() / \"repos\" / \"fourier-head\" / \"notebooks\"\n",
    "sys.path.insert(0, str(repo_dir))\n",
    "\n",
    "from GMM_head_2D_logsum import GMMHead2D   # numerically-stable GMM head\n",
    "\n",
    "# ─── 2) Helper: read cleaned AIS parquet into a Polars DF ───────────────────\n",
    "def load_cleaned_data(root: str) -> pl.DataFrame:\n",
    "    p = Path(root).expanduser().resolve()\n",
    "    files = list(p.rglob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files under {p}\")\n",
    "    df = pl.read_parquet([str(f) for f in files])\n",
    "    print(f\"Loaded {df.height:,} rows from {len(files)} parquet files\")\n",
    "    return df\n",
    "\n",
    "# ─── 3) Streaming dataset ───────────────────────────────────────────────────\n",
    "class AISForecastIterableDataset(IterableDataset):\n",
    "    def __init__(self, df: pl.DataFrame, seq_len: int = 20):\n",
    "        import time\n",
    "        t0 = time.time()\n",
    "\n",
    "        # (No Config call needed; threading already active)\n",
    "\n",
    "        df = (\n",
    "            df.with_columns([\n",
    "                (pl.col(\"lat\") / 90).alias(\"lat_n\"),\n",
    "                (pl.col(\"lon\") / 180).alias(\"lon_n\"),\n",
    "            ])\n",
    "            .sort([\"mmsi\", \"timestamp\"])        # heavy but threaded\n",
    "        )\n",
    "        self.df     = df\n",
    "        self.mmsis  = df[\"mmsi\"].unique().to_list()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        print(f\"🟢 Dataset ready: {len(self.mmsis):,} vessels \"\n",
    "              f\"in {time.time()-t0:.1f}s\", flush=True)\n",
    "\n",
    "# ─── 4) Model: Transformer encoder → GMM head ───────────────────────────────\n",
    "class TransformerForecasterGMM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        num_layers: int,\n",
    "        ff_hidden: int,\n",
    "        num_components: int,\n",
    "        device: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(2, d_model)\n",
    "        self.pos_emb    = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        enc_layer       = nn.TransformerEncoderLayer(d_model, nhead, ff_hidden)\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n",
    "        self.gmm = GMMHead2D(\n",
    "            dim_input=d_model,\n",
    "            num_components=num_components,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: torch.Tensor):\n",
    "        dev = self.pos_emb.device\n",
    "        x       = x.to(dev, non_blocking=True)\n",
    "        targets = targets.to(dev, non_blocking=True)\n",
    "\n",
    "        h = self.input_proj(x) + self.pos_emb.unsqueeze(0)   # (B,S,d)\n",
    "        h = self.transformer(h.transpose(0, 1))              # (S,B,d)\n",
    "        last = h[-1]                                         # (B,d)\n",
    "        return self.gmm(last, targets, return_params=True)   # log_p, means\n",
    "\n",
    "# ─── 5) Training loop with blended loss ─────────────────────────────────────\n",
    "def train_gmm(\n",
    "    df: pl.DataFrame,\n",
    "    *,\n",
    "    seq_len: int = 20,\n",
    "    d_model: int = 128,\n",
    "    nhead: int = 4,\n",
    "    num_layers: int = 4,\n",
    "    ff_hidden: int = 512,\n",
    "    num_components: int = 8,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 3e-4,\n",
    "    epochs: int = 100,\n",
    "    mse_weight: float = 0.1,           # λ\n",
    "    device: Optional[str] = None,\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ds = AISForecastIterableDataset(df, seq_len)\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size, shuffle=False, drop_last=True,\n",
    "        pin_memory=(device == \"cuda\"),\n",
    "    )\n",
    "    model = TransformerForecasterGMM(\n",
    "        seq_len, d_model, nhead, num_layers,\n",
    "        ff_hidden, num_components, device\n",
    "    ).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tot_loss = tot_nll = tot_mse = seen = 0\n",
    "        for xb, yb in loader:\n",
    "            log_p, means = model(xb, yb)          # tuple\n",
    "            nll  = -log_p.mean()\n",
    "            mse  = ((means - yb.to(device).unsqueeze(1))**2).mean()\n",
    "            loss = nll + mse_weight * mse\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            bsz = xb.size(0)\n",
    "            tot_loss += loss.item() * bsz\n",
    "            tot_nll  += nll.item()  * bsz\n",
    "            tot_mse  += mse.item()  * bsz\n",
    "            seen     += bsz\n",
    "\n",
    "        print(f\"[Ep {ep:03}]  loss {tot_loss/seen:6.3f}  \"\n",
    "              f\"NLL {tot_nll/seen:6.3f}  MSE {tot_mse/seen:6.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ─── 6) Execute end-to-end ──────────────────────────────────────────────────\n",
    "df = load_cleaned_data(\"data/cleaned_partitioned_ais\")\n",
    "model = train_gmm(\n",
    "    df,\n",
    "    seq_len        = 20,\n",
    "    d_model        = 128,\n",
    "    nhead          = 4,\n",
    "    num_layers     = 4,\n",
    "    ff_hidden      = 512,\n",
    "    num_components = 8,\n",
    "    batch_size     = 64,\n",
    "    lr             = 3e-4,\n",
    "    epochs         = 100,\n",
    "    mse_weight     = 0.1,            # λ\n",
    ")\n",
    "print(\"✅  Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199604b-4a20-44c9-b3cb-1726d2b1cc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
