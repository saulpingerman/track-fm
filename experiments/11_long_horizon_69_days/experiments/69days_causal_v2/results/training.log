
Experiment: 69days_causal_v2
Output directory: /home/ec2-user/trackfm-toy studies/experiments/11_long_horizon_69_days/experiments/69days_causal_v2
  Checkpoints: /home/ec2-user/trackfm-toy studies/experiments/11_long_horizon_69_days/experiments/69days_causal_v2/checkpoints
  Results: /home/ec2-user/trackfm-toy studies/experiments/11_long_horizon_69_days/experiments/69days_causal_v2/results
  Log file: /home/ec2-user/trackfm-toy studies/experiments/11_long_horizon_69_days/experiments/69days_causal_v2/results/training.log

Configuration:
  data_path: /mnt/fsx/data
  catalog_path: /mnt/fsx/data/track_catalog.parquet
  min_track_length: 600
  max_seq_len: 128
  min_sog: 3.0
  d_model: 384
  nhead: 16
  num_layers: 8
  dim_feedforward: 2048
  dropout: 0.1
  grid_size: 64
  num_freqs: 12
  grid_range: 0.3
  max_horizon: 400
  num_horizon_samples: 8
  batch_size: 8000
  learning_rate: 0.0003
  weight_decay: 1e-05
  num_epochs: 100
  warmup_steps: 20
  val_every_n_batches: 20
  val_max_batches: 10
  sigma: 0.003
  dr_sigma: 0.05
  early_stop_patience: 4
  early_stop_min_delta: 0.01
  use_amp: True
  num_workers: 0
  pin_memory: True
  gradient_accumulation: 1
  lat_mean: 56.25
  lat_std: 1.0
  lon_mean: 11.5
  lon_std: 2.0
  sog_max: 30.0
  dt_max: 300.0

======================================================================
LOADING DATA
======================================================================
Using EAGER LOADING mode (all data in memory)
Loading track catalog...
  Selected 126903 tracks for training
Loading track data from FSx (streaming)...
    Processed tracks.parquet, tracks so far: 644
    Processed tracks.parquet, tracks so far: 1095
    Processed tracks.parquet, tracks so far: 1416
    Processed tracks.parquet, tracks so far: 1800
    Processed tracks.parquet, tracks so far: 2161
    Processed tracks.parquet, tracks so far: 2450
    Processed tracks.parquet, tracks so far: 2654
    Processed tracks.parquet, tracks so far: 2997
    Processed tracks.parquet, tracks so far: 3482
    Processed tracks.parquet, tracks so far: 3824
    Processed tracks.parquet, tracks so far: 4120
    Processed tracks.parquet, tracks so far: 4542
    Processed tracks.parquet, tracks so far: 4837
    Processed tracks.parquet, tracks so far: 5168
    Processed tracks.parquet, tracks so far: 5603
    Processed tracks.parquet, tracks so far: 5987
    Processed tracks.parquet, tracks so far: 6336
    Processed tracks.parquet, tracks so far: 6780
    Processed tracks.parquet, tracks so far: 7161
    Processed tracks.parquet, tracks so far: 7525
    Processed tracks.parquet, tracks so far: 7861
    Processed tracks.parquet, tracks so far: 8247
    Processed tracks.parquet, tracks so far: 8572
    Processed tracks.parquet, tracks so far: 8869
    Processed tracks.parquet, tracks so far: 9189
    Processed tracks.parquet, tracks so far: 9499
    Processed tracks.parquet, tracks so far: 9787
    Processed tracks.parquet, tracks so far: 10165
    Processed tracks.parquet, tracks so far: 10520
    Processed tracks.parquet, tracks so far: 10864
    Processed tracks.parquet, tracks so far: 11202
    Processed tracks.parquet, tracks so far: 11557
    Processed tracks.parquet, tracks so far: 11901
    Processed tracks.parquet, tracks so far: 12237
    Processed tracks.parquet, tracks so far: 12568
    Processed tracks.parquet, tracks so far: 12877
    Processed tracks.parquet, tracks so far: 13279
    Processed tracks.parquet, tracks so far: 13513
  Loaded 13513 tracks in 31.1s
  Total positions: 140,613,773
  Train tracks: 10204, Val tracks: 1134

Creating datasets...
  Created 3,784,235 training samples from 10204 tracks
  Created 428,705 training samples from 1134 tracks
  Limited validation to 40,000 samples
  Train batches: 474
  Val batches: 40

======================================================================
CREATING MODEL
======================================================================
  Parameters: 18,273,378
  Architecture: d_model=384, nhead=16, num_layers=8, dim_ff=2048

======================================================================
FINDING OPTIMAL BATCH SIZE
======================================================================

Finding optimal batch size for 90% GPU utilization...
  Total GPU memory: 47.7 GB
  Target memory: 42.9 GB
  Batch size 8032: OOM
  Batch size 4047: OOM
  Batch size 2055: OOM
  Batch size 1059: OOM
  Batch size 561: 46.0 GB (96.4%) (too high)
  Batch size 312: 11.4 GB (23.9%) ✓
  Batch size 436: 16.4 GB (34.4%) ✓
  Batch size 498: 23.5 GB (49.3%) ✓
  Batch size 529: 12.1 GB (25.5%) ✓
  Batch size 545: 14.5 GB (30.5%) ✓
  Batch size 553: 25.5 GB (53.5%) ✓
  Batch size 557: 23.7 GB (49.6%) ✓
  Batch size 559: 18.2 GB (38.3%) ✓
  Batch size 560: 22.5 GB (47.1%) ✓

  Optimal batch size: 500

Recreating data loaders with batch_size=500...
  Train batches: 7569
  Val batches: 80

======================================================================
TRAINING
======================================================================

Initial validation (before training)...
  Model (untrained): 22.2346
  Random Model:      21.9155
  Dead Reckoning:    5.2763
  Last Position:     8.2689

  >>> VALIDATION at step 1 (batch 1):
      Train Loss:     22.0919
      ---
      Model:          22.2346  (vs DR: -321.4%, vs LP: -168.9%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_1.pt
      New best model!

  >>> VALIDATION at step 3 (batch 3):
      Train Loss:     23.2344
      ---
      Model:          16.2554  (vs DR: -208.1%, vs LP: -96.6%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_3.pt
      New best model!

  >>> VALIDATION at step 7 (batch 7):
      Train Loss:     16.6772
      ---
      Model:          8.8609  (vs DR: -67.9%, vs LP: -7.2%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_7.pt
      New best model!
  Epoch 1, Batch 10/7569, Loss: 11.3864, LR: 1.50e-04, GPU: 22.5GB

  >>> VALIDATION at step 15 (batch 15):
      Train Loss:     13.9778
      ---
      Model:          14.2632  (vs DR: -170.3%, vs LP: -72.5%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_15.pt
      No improvement (1/4)
  Epoch 1, Batch 20/7569, Loss: 11.2532, LR: 3.00e-04, GPU: 25.3GB
  Epoch 1, Batch 30/7569, Loss: 9.9473, LR: 3.00e-04, GPU: 29.0GB

  >>> VALIDATION at step 31 (batch 31):
      Train Loss:     9.8464
      ---
      Model:          5.0566  (vs DR: +4.2%, vs LP: +38.8%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_31.pt
      New best model!
  Epoch 1, Batch 40/7569, Loss: 9.1157, LR: 3.00e-04, GPU: 29.0GB
  Epoch 1, Batch 50/7569, Loss: 7.7136, LR: 3.00e-04, GPU: 29.0GB
  Epoch 1, Batch 60/7569, Loss: 6.5509, LR: 3.00e-04, GPU: 29.0GB

  >>> VALIDATION at step 63 (batch 63):
      Train Loss:     6.2723
      ---
      Model:          2.6424  (vs DR: +49.9%, vs LP: +68.0%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_63.pt
      New best model!
  Epoch 1, Batch 70/7569, Loss: 2.1744, LR: 3.00e-04, GPU: 33.5GB
  Epoch 1, Batch 80/7569, Loss: 3.2323, LR: 3.00e-04, GPU: 33.5GB
  Epoch 1, Batch 90/7569, Loss: 3.1065, LR: 3.00e-04, GPU: 33.5GB
  Epoch 1, Batch 100/7569, Loss: 3.1887, LR: 3.00e-04, GPU: 33.5GB
  Epoch 1, Batch 110/7569, Loss: 3.0199, LR: 3.00e-04, GPU: 33.5GB
  Epoch 1, Batch 120/7569, Loss: 3.0724, LR: 3.00e-04, GPU: 33.5GB

  >>> VALIDATION at step 127 (batch 127):
      Train Loss:     3.1347
      ---
      Model:          2.3871  (vs DR: +54.8%, vs LP: +71.1%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_127.pt
      New best model!
  Epoch 1, Batch 130/7569, Loss: 6.0971, LR: 3.00e-04, GPU: 33.5GB
  Epoch 1, Batch 140/7569, Loss: 3.2712, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 150/7569, Loss: 2.8638, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 160/7569, Loss: 2.9946, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 170/7569, Loss: 3.1715, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 180/7569, Loss: 3.1186, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 190/7569, Loss: 3.0256, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 200/7569, Loss: 2.9740, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 210/7569, Loss: 2.9818, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 220/7569, Loss: 2.9779, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 230/7569, Loss: 2.9818, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 240/7569, Loss: 2.9590, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 250/7569, Loss: 2.9630, LR: 3.00e-04, GPU: 34.8GB

  >>> VALIDATION at step 255 (batch 255):
      Train Loss:     2.9316
      ---
      Model:          1.6684  (vs DR: +68.4%, vs LP: +79.8%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_255.pt
      New best model!
  Epoch 1, Batch 260/7569, Loss: 4.0029, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 270/7569, Loss: 3.4054, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 280/7569, Loss: 3.2808, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 290/7569, Loss: 3.1219, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 300/7569, Loss: 2.9132, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 310/7569, Loss: 2.8426, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 320/7569, Loss: 2.8092, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 330/7569, Loss: 2.8295, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 340/7569, Loss: 2.8068, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 350/7569, Loss: 2.6644, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 360/7569, Loss: 2.5684, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 370/7569, Loss: 2.5319, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 380/7569, Loss: 2.5452, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 390/7569, Loss: 2.5395, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 400/7569, Loss: 2.5167, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 410/7569, Loss: 2.5077, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 420/7569, Loss: 2.5023, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 430/7569, Loss: 2.4769, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 440/7569, Loss: 2.4611, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 450/7569, Loss: 2.4481, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 460/7569, Loss: 2.4360, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 470/7569, Loss: 2.4001, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 480/7569, Loss: 2.3690, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 490/7569, Loss: 2.3774, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 500/7569, Loss: 2.3750, LR: 3.00e-04, GPU: 34.8GB
  Epoch 1, Batch 510/7569, Loss: 2.3602, LR: 3.00e-04, GPU: 36.7GB

  >>> VALIDATION at step 511 (batch 511):
      Train Loss:     2.3621
      ---
      Model:          1.6941  (vs DR: +67.9%, vs LP: +79.5%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_511.pt
      No improvement (1/4)
  Epoch 1, Batch 520/7569, Loss: 3.1804, LR: 3.00e-04, GPU: 36.7GB
  Epoch 1, Batch 530/7569, Loss: 2.8941, LR: 3.00e-04, GPU: 36.7GB
  Epoch 1, Batch 540/7569, Loss: 2.7074, LR: 3.00e-04, GPU: 36.7GB
  Epoch 1, Batch 550/7569, Loss: 2.6962, LR: 3.00e-04, GPU: 36.7GB
  Epoch 1, Batch 560/7569, Loss: 2.6220, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 570/7569, Loss: 2.4916, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 580/7569, Loss: 2.3853, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 590/7569, Loss: 2.3609, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 600/7569, Loss: 2.3100, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 610/7569, Loss: 2.3416, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 620/7569, Loss: 2.2990, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 630/7569, Loss: 2.2238, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 640/7569, Loss: 2.1786, LR: 3.00e-04, GPU: 39.0GB
  Epoch 1, Batch 650/7569, Loss: 2.1944, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 660/7569, Loss: 2.2189, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 670/7569, Loss: 2.2047, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 680/7569, Loss: 2.1738, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 690/7569, Loss: 2.1422, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 700/7569, Loss: 2.1221, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 710/7569, Loss: 2.1073, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 720/7569, Loss: 2.1375, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 730/7569, Loss: 2.1204, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 740/7569, Loss: 2.0905, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 750/7569, Loss: 2.0697, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 760/7569, Loss: 2.0363, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 770/7569, Loss: 2.0069, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 780/7569, Loss: 1.9947, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 790/7569, Loss: 1.9845, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 800/7569, Loss: 1.9645, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 810/7569, Loss: 1.9664, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 820/7569, Loss: 1.9679, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 830/7569, Loss: 1.9607, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 840/7569, Loss: 1.9431, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 850/7569, Loss: 1.9603, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 860/7569, Loss: 1.9427, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 870/7569, Loss: 1.9609, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 880/7569, Loss: 1.9587, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 890/7569, Loss: 1.9406, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 900/7569, Loss: 1.9286, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 910/7569, Loss: 1.9160, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 920/7569, Loss: 1.9186, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 930/7569, Loss: 1.9174, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 940/7569, Loss: 1.9094, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 950/7569, Loss: 1.9128, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 960/7569, Loss: 1.9178, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 970/7569, Loss: 1.9115, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 980/7569, Loss: 1.9056, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 990/7569, Loss: 1.9147, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1000/7569, Loss: 1.9320, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1010/7569, Loss: 1.9308, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1020/7569, Loss: 1.9341, LR: 3.00e-04, GPU: 39.2GB

  >>> VALIDATION at step 1023 (batch 1023):
      Train Loss:     1.9449
      ---
      Model:          1.3557  (vs DR: +74.3%, vs LP: +83.6%)
      Dead Reckoning: 5.2763
      Last Position:  8.2689
      Random Model:   21.9155

      Saved checkpoint: checkpoint_step_1023.pt
      New best model!
  Epoch 1, Batch 1030/7569, Loss: 2.0133, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1040/7569, Loss: 2.2871, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1050/7569, Loss: 2.0352, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1060/7569, Loss: 1.8872, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1070/7569, Loss: 1.7263, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1080/7569, Loss: 1.7993, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1090/7569, Loss: 1.7844, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1100/7569, Loss: 1.8456, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1110/7569, Loss: 1.9395, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1120/7569, Loss: 1.9874, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1130/7569, Loss: 1.9822, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1140/7569, Loss: 1.9362, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1150/7569, Loss: 1.9122, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1160/7569, Loss: 1.9141, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1170/7569, Loss: 1.8986, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1180/7569, Loss: 1.8401, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1190/7569, Loss: 1.8197, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1200/7569, Loss: 1.8262, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1210/7569, Loss: 1.8533, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1220/7569, Loss: 1.8659, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1230/7569, Loss: 1.8413, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1240/7569, Loss: 1.8082, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1250/7569, Loss: 1.8369, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1260/7569, Loss: 1.8280, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1270/7569, Loss: 1.8164, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1280/7569, Loss: 1.7993, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1290/7569, Loss: 1.8090, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1300/7569, Loss: 1.8226, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1310/7569, Loss: 1.8101, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1320/7569, Loss: 1.8124, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1330/7569, Loss: 1.7974, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1340/7569, Loss: 1.7738, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1350/7569, Loss: 1.7697, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1360/7569, Loss: 1.7705, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1370/7569, Loss: 1.7788, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1380/7569, Loss: 1.7606, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1390/7569, Loss: 1.7514, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1400/7569, Loss: 1.7347, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1410/7569, Loss: 1.7242, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1420/7569, Loss: 1.7220, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1430/7569, Loss: 1.7242, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1440/7569, Loss: 1.7254, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1450/7569, Loss: 1.7454, LR: 3.00e-04, GPU: 39.2GB
  Epoch 1, Batch 1460/7569, Loss: 1.7465, LR: 3.00e-04, GPU: 39.2GB
